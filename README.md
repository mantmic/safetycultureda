# SafetyCulture Data Engineering Challenge

## Overview
This repository is for the SafetyCulture Data Engineering challenge.

Docker is used to spin up a number of services for the data pipeline.

The purpose of each service is as follows;

| Service  | Description |
| ------------- | ------------- |
| postgres  | The postgres database used for this data pipeline |
| postgres-seed  | A service that creates database tables, inserts raw example data into the postgres database, then runs the ETLs to populate the dimension tables. This runs once when the services are started, then stops.  |
| message-broker  | Message broker service that receives user event data  |
| user-event  | Service that pulls user-event data from the message-broker service and inserts the data into the postgres database |
| user-event-seed  | A service that pushes example user-event data onto the message broker. This runs once when the services are started, then stops.  |

## Requirements

* docker
* docker-compose
* R (for the head-end example)

## General architecture

The following diagram shows the architecture of this data stream.

![architecture diagram](https://raw.githubusercontent.com/mantmic/safetycultureda/master/images/diagram.png "Data architecture")


User metadata is pushed directly into the database. User events are retrieved via a message broker.

In this example, the postgres-seed service will populate the staging tables, simulating data landing in the database from a foreign system, then it will run the ETL procedure to populate the sc.sc_user table.

The user-event-seed service will push messages onto the message broker, which will be automatically processed by the user-event service and pushed into the database.

View the logs of the user-event service to see it process each individual message.
```
docker-compose logs user-event
```

The postgres database can be connected to and explored locally via port 5432.

```
PGPASSWORD=postgres psql -h localhost -U postgres postgres
```

## Running

Run the following to pull the repository, pull build the docker images, and start the services.

```
git clone https://github.com/mantmic/safetyculturede
cd safetyculturede
docker-compose build
docker-compose up -d
```

### Head-end example

To show an example on how a data analyst may connect to the database to perform some analysis, the headend-example directory contains an R script that connects to the database and generates a report.

From the repository directory this report can be generated by running the [R Markdown Script](headend-example/Headend Example.Rmd) in R Studio.

This has been already generated and can be found [here](headend-example/Headend_Example.html)

#### R Package requirements
* RPostgreSQL
* ggplot2

## Database structure

### Dimensions

The SafetyCulture software is used as the master system for this data warehouse. The user table in software is based on users of the SafetyCulture application ecosystem. ETLs attend to associate users from other systems (e.g. CRM) to a SafetyCulture user.

All dimensional user data (data about the user, not their real-time actions), lands in the staging schema in the database.

An ETL process integrates user data from various sources into a single table with all information on the customer, sc.sc_user.

As a user can move between companies but retain their SafetyCulture account, history must be kept so that user behaviour is associated with the correct organisation. As a result, historical changes for the customer are tracked in sc.sc_user_hist.


### Real-time customer events

As this is where most of the data volume will come from, the data will require minimal manipulation, and there may be real-time analytics requirements, user event data will be directly inserted into the sc.sc_user_event table.

### Tables

| Schema  | Table | Description |
| ------------- | ------------- | ------------- |
| staging  | sc_user_document | Raw SafetyCulture user data in a NoSQL / JSON nested structure |
| staging  | crm_customer | Raw output of crm customer data |
| sc  | sc_user | User dimension containing information user/customer information extracted from multiple sources |
| sc  | sc_user_hist | Table that tracks historical changes to sc.sc_user |
| sc  | sc_user_event | Table containing real-time user event data |


## Scalability & future changes

The architecture used was designed to mimic a production data pipeline architecture. A production solution would likely have the following changes.

### User event data pipeline

This script is not optimised for large data volumes, ideally a script would use a database connection pool, and maintain a listening connection to the message broker. For the sake of simplicity, in this example the python script checks for new messages every 10 seconds, and inserts them individually one by one.

The message broker and user-event services wouldn't directly insert data into the database from the message broker. User event data may need to go through some minor processing + validation prior to being usable by any recipient. In a production solution there may be multiple recipients for this data.

For example, real-time analytics may be running over the message stream, and would require the same data validation as the database would.

A simple alteration would be to have two queues in the message broker. One queue receives user-event data. A service will pick up messages, validate + restructure messages, then push them onto a second queue.

The second queue would have potentially multiple recipients, including a service to pick up messages and insert them into the database.

### User metadata pipeline

The pipeline for user metadata assumes external scripts will insert directly into the staging tables. That tightly couples procedures owned by the teams running the external systems with the data analytics stack.

Ideally an inbound API would be developed to decouple the database stack with the data upload procedures. Changes in the underlying database technology can be made without upstream scripts being impacted.
